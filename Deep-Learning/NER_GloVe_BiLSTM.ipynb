{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition with GloVe-BiLSTM-Softmax model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as progress\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import rnn\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Read and process NER 2003 English Shared Task data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fpath):\n",
    "    \"\"\" Read NER 2003 English Shared Task data (CoNNL file format)\n",
    "    Data format:\n",
    "        All data files contain one word per line with empty lines\n",
    "        representing sentence boundaries. At the end of each line there is \n",
    "        tag which states whether the current word is inside a named entity or not.\n",
    "        The tag also encodes the type of named entity. Example\n",
    "        Example:\n",
    "            U.N. NNP I-NP I-ORG\n",
    "        Each line contains four fields:\n",
    "            [word] [POS tag] [chunk tag] [NE tag]\n",
    "            \n",
    "        Four different types of named entities: PERSON, LOCATION, ORGANIZATION, MISC.\n",
    "        \n",
    "    Args:\n",
    "        fpath: path to data file\n",
    "    Returns:\n",
    "        sentences_words: list of sentences' words (one sentence is a list of words)\n",
    "        sentences_tags: list of sentences' tags (one sentence is a list of tags corresponding to words)\n",
    "    \"\"\"\n",
    "    with open(fpath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    raw_sentences = []\n",
    "    i_prev, i_next = 0, 0\n",
    "    while i_next < len(lines):\n",
    "        if lines[i_next] == '\\n':\n",
    "            raw_sentences.append(lines[i_prev:i_next])\n",
    "            i_prev = i_next + 1\n",
    "        i_next += 1\n",
    "\n",
    "    sentences_words = []\n",
    "    sentences_tags = []\n",
    "    for sentence in raw_sentences:\n",
    "        words = [string.split()[0] for string in sentence]\n",
    "        tags = [string.split()[3] for string in sentence]\n",
    "        if words != ['-DOCSTART-']:\n",
    "            sentences_words.append(words)\n",
    "            sentences_tags.append(tags)\n",
    "    return list(zip(sentences_words, sentences_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train (14041), dev (3250), test (3453) data\n"
     ]
    }
   ],
   "source": [
    "root = './data'\n",
    "\n",
    "train_data = read_data('./data/train.txt')\n",
    "val_data = read_data('./data/dev.txt')\n",
    "test_data = read_data('./data/test.txt')\n",
    "\n",
    "print('Loaded train ({}), dev ({}), test ({}) data'.format(len(train_data),\n",
    "                                                          len(val_data),\n",
    "                                                          len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30289 words in vocabulary\n",
      "9 tags (IOB2 tagging scheme): dict_keys(['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC'])\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {}  # vocabulary\n",
    "tag_to_idx = {}  # tagset\n",
    "\n",
    "for i, (sentence, tags) in enumerate(train_data + val_data + test_data):\n",
    "    for word in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_idx:\n",
    "            tag_to_idx[tag] = len(tag_to_idx)\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "print(vocab_size, 'words in vocabulary')\n",
    "\n",
    "tagset_size = len(tag_to_idx)\n",
    "print(tagset_size, 'tags (IOB2 tagging scheme):', tag_to_idx.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccda1e6e5bde4c329e1137cc3d714924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GloVe for 400000 words loaded\n"
     ]
    }
   ],
   "source": [
    "with open('./glove/glove.6B.100d.txt', 'r') as f:\n",
    "    glove_raw = f.readlines()\n",
    "    \n",
    "glove = {line.split()[0]: torch.tensor([float(val) for val in line.split()[1:]])\n",
    "         for line in progress(glove_raw)}\n",
    "\n",
    "print('GloVe for {} words loaded'.format(len(glove)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe embeddings as dict vocabulary and embedding matrix of shape (30289, 100)\n",
      "Example embedding for `the`:\n",
      " tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n",
      "        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n",
      "         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n",
      "         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n",
      "         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n",
      "        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n",
      "        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n",
      "         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n",
      "         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n",
      "        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n",
      "         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n",
      "         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n",
      "        -0.5203, -0.1459,  0.8278,  0.2706])\n"
     ]
    }
   ],
   "source": [
    "def get_gloves(seq, glove, strategy=3):\n",
    "    \"\"\"Get Global Vectors for Word Representation in a list of words\n",
    "    Args:\n",
    "        seq: list of words of size N\n",
    "        glove: GloVe dictionary of 400000 words, the format\n",
    "               {word: 100D vector representation}\n",
    "        strategy: 1, 2 or 3\n",
    "            1 -- load the embeddings for original capitalization of words.\n",
    "                 If embedding for this word doesn’t exists, associate\n",
    "                 it with <UNK> embedding.\n",
    "            2 -- load the embeddings for lowercased capitalization of words.\n",
    "                 If embedding for this lowercased word doesn’t exists,\n",
    "                 associate it with <UNK> embedding.\n",
    "            3 -- Для різної капіталізації слів завантажуємо одні і ті самі (lowercased)\n",
    "                 вектори ембедінгів, але в нашому словнику це різні слова.\n",
    "                 Тобто, “Hello” та “hello” відповідають фізично два вектори,\n",
    "                 які ідентичні за своїми значеннями перед тренуванням\n",
    "                 (якщо freeze_embeddings стоїть  False, то в процесі\n",
    "                 тренування вони будуть змінюватись).\n",
    "    Returns:\n",
    "        out_seq: a list of [100] tensors (GloVes) of size N\n",
    "    \"\"\"\n",
    "    assert strategy in [1,2,3]\n",
    "    \n",
    "    def associate_embedding(word):\n",
    "        if strategy == 1:\n",
    "            return glove['unk'] if word not in glove else glove[word]\n",
    "        if strategy == 2:\n",
    "            return glove['unk'] if word.lower() not in glove else glove[word.lower()]\n",
    "        return glove['unk'] if word.lower() not in glove else \\\n",
    "                glove[word.lower()] if word not in glove else glove[word]\n",
    "    \n",
    "    out_seq = [associate_embedding(w) for w in seq]\n",
    "    return out_seq, torch.stack(out_seq, 0)\n",
    "\n",
    "embedding_dim = 100\n",
    "strategy = 3\n",
    "\n",
    "gloves, gloves_matrix = get_gloves(word_to_idx.keys(), glove, strategy)\n",
    "word_to_glove = dict(zip(word_to_idx.keys(), gloves))\n",
    "print('Loaded GloVe embeddings as dict vocabulary and embedding matrix of shape',\n",
    "          tuple(gloves_matrix.shape))\n",
    "\n",
    "print('Example embedding for `the`:\\n', word_to_glove['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-4. Train BiLSTM model on batches & test with micro-average Precision/Recall/F1/F0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        embedding_dim -- 100\n",
    "        hidden_dim -- hidden state dimensionality\n",
    "        vocab_size -- vocabulary size\n",
    "        tagset_size -- tag set size\n",
    "        pretrained_embeddings -- None or [vocab_size, embedding_dim] tensor\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size,\n",
    "                 pretrained_embeddings=None):\n",
    "\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.word_embeddings = nn.Embedding.from_pretrained(pretrained_embeddings,\n",
    "                                                                freeze=False)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2,\n",
    "                            bidirectional=True, batch_first=True)\n",
    "        self.tagger = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs, packed_inputs=False):\n",
    "        if packed_inputs:\n",
    "            embeds = rnn.PackedSequence(self.word_embeddings(inputs.data), inputs.batch_sizes)\n",
    "        else:\n",
    "            embeds = self.word_embeddings(inputs).view(len(inputs), 1, -1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "\n",
    "        if packed_inputs:\n",
    "            tag_space = self.tagger(lstm_out.data)\n",
    "        else:\n",
    "            tag_space = self.tagger(lstm_out.view(len(inputs), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def seq_to_idxs(seq, mapping):\n",
    "    \"\"\"Converts a sequence of elements to sequence of indices\n",
    "       using given mapping\n",
    "\n",
    "    Args:\n",
    "        seq -- list of elements\n",
    "        mapping -- {element: idx} dict\n",
    "    \"\"\"\n",
    "    outs_seq = [torch.tensor(mapping[el], dtype=torch.long) for el in seq]\n",
    "    outs_seq = torch.stack(outs_seq, 0)\n",
    "    return outs_seq\n",
    "\n",
    "\n",
    "def calculate_scores(outputs, targets, log=False):\n",
    "    \"\"\"Calculate per-class and micro-average precision, recall, F-1 score and F-0.5 score\n",
    "    \n",
    "        Args:\n",
    "            outputs: tensor of log-softmax model outputs for the dataset\n",
    "            targets: tensor of true tag indices for the dataset\n",
    "            log: whether to print or not\n",
    "        Returns:\n",
    "            (micro-average) precision, recall, F-1 score, F-0.5 score\n",
    "    \"\"\"    \n",
    "    def F_score(precision, recall, beta=1):\n",
    "        return (1 + beta**2) * precision * recall / ((beta**2) * precision + recall + 1e-15)\n",
    "\n",
    "    pred = outputs.max(dim=1)[1]\n",
    "    TP, TN, FP, FN = torch.zeros(9), torch.zeros(9), torch.zeros(9), torch.zeros(9)\n",
    "    if log:\n",
    "        print('Tag\\tSize\\tPrecision\\tRecall\\t\\tF1\\tF0.5')\n",
    "    for tag in tag_to_idx.keys():\n",
    "        i = tag_to_idx[tag]\n",
    "        TP[i] = (pred[targets==i]==i).sum()\n",
    "        TN[i] = (pred[targets!=i]!=i).sum()\n",
    "        FP[i] = (pred[targets!=i]==i).sum()\n",
    "        FN[i] = (pred[targets==i]!=i).sum()\n",
    "\n",
    "        precision = float(TP[i] / (TP[i] + FP[i] + 1e-15))\n",
    "        recall = float(TP[i] / (TP[i] + FN[i] + 1e-15))\n",
    "        if log:\n",
    "            print('{}\\t{}\\t{:.4f}\\t\\t{:.4f}\\t\\t{:.4f}\\t{:.4f}'.format(tag, len(targets[targets==i]),\n",
    "                                                                    precision, recall,\n",
    "                                                                    F_score(precision, recall, beta=1),\n",
    "                                                                    F_score(precision, recall, beta=0.5)))\n",
    "\n",
    "    MicroAvePrecision = float(TP.sum() / (TP.sum() + FP.sum()))\n",
    "    MicroAveRecall = float(TP.sum() / (TP.sum() + FN.sum()))\n",
    "    F1 = F_score(MicroAvePrecision, MicroAveRecall, beta=1)\n",
    "    F05 = F_score(MicroAvePrecision, MicroAveRecall, beta=0.5)\n",
    "    if log:\n",
    "        print('{}\\t{}\\t{:.4f}\\t\\t{:.4f}\\t\\t{:.4f}\\t{:.4f}'.format('MICRO', len(targets),\n",
    "                                                                  MicroAvePrecision,\n",
    "                                                                  MicroAveRecall,\n",
    "                                                                  F1, F05))\n",
    "    return MicroAvePrecision, MicroAveRecall, F1, F05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, loss_fn, optimizer, word_to_idx, tag_to_idx):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "    \n",
    "    def run(self, num_epochs,\n",
    "            train_data, train_batch_size, val_data, val_batch_size,\n",
    "            log_interval=50):\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            self.__train(epoch, train_data, train_batch_size, log_interval)\n",
    "            self.__validate(epoch, val_data, val_batch_size)\n",
    "        \n",
    "    def __train(self, epoch, train_data, batch_size, log_interval=50):\n",
    "        losses = AverageMeter()\n",
    "        batch_time = AverageMeter()\n",
    "        batch_start = 0\n",
    "        batch_idx = 0\n",
    "        while batch_start < len(train_data):\n",
    "            end = time.time()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            batch = train_data[batch_start:batch_start + batch_size]\n",
    "            inputs_packed, targets_packed = self._prepare_batch(batch)\n",
    "            outputs = self.model(inputs_packed, True)\n",
    "\n",
    "            loss = self.loss_fn(outputs, targets_packed.data)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            losses.update(loss.item())\n",
    "            batch_time.update(time.time() - end)\n",
    "\n",
    "            batch_start += batch_size\n",
    "            batch_idx += 1\n",
    "            \n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {}\\t[{:>5}/{:<5}]\\tTime: {:.2f} ({:.2f})\\tLoss: {:.4f} ({:.4f})'.format(epoch,\n",
    "                    min(batch_start, len(train_data)), len(train_data),\n",
    "                    batch_time.val, batch_time.avg,\n",
    "                    losses.val, losses.avg))\n",
    "        print('====> Train. {}\\tTotal time: {:.2f}\\tAverage loss: {:.4f}'.format(\n",
    "              epoch, batch_time.sum, losses.avg))\n",
    "            \n",
    "            \n",
    "    def __validate(self, epoch, val_data, batch_size):\n",
    "        losses = AverageMeter()\n",
    "        batch_time = AverageMeter()\n",
    "        F1scores = AverageMeter()\n",
    "        F05scores = AverageMeter()\n",
    "        with torch.no_grad():\n",
    "            batch_start = 0\n",
    "            batch_idx = 0\n",
    "            while batch_start < len(val_data):\n",
    "                end = time.time()\n",
    "\n",
    "                batch = val_data[batch_start:batch_start + batch_size]\n",
    "                inputs_packed, targets_packed = self._prepare_batch(batch)\n",
    "                outputs = self.model(inputs_packed, True)\n",
    "\n",
    "                loss = self.loss_fn(outputs, targets_packed.data)\n",
    "                _, _, F1, F05 = calculate_scores(outputs, targets_packed.data, log=False)\n",
    "                F1scores.update(F1)\n",
    "                F05scores.update(F05)\n",
    "\n",
    "                losses.update(loss.item())\n",
    "                batch_time.update(time.time() - end)\n",
    "\n",
    "                batch_start += batch_size\n",
    "                batch_idx += 1\n",
    "        print('====> Valid. {}\\tTotal time: {:.2f}\\tAverage loss: {:.4f}\\tF-1: {:.4f}\\tF-0.5: {:.4f}\\t'.format(\n",
    "              epoch, batch_time.sum, losses.avg, F1scores.avg, F05scores.avg))\n",
    "            \n",
    "    def _prepare_batch(self, batch):\n",
    "        inputs_batch = [seq_to_idxs(seq[0], self.word_to_idx) for seq in batch]\n",
    "        targets_batch = [seq_to_idxs(seq[1], self.tag_to_idx) for seq in batch]\n",
    "\n",
    "        order = sorted(enumerate(inputs_batch), key=lambda x: len(x[1]), reverse=True)\n",
    "        inputs_batch = [inputs_batch[order_[0]] for order_ in order]\n",
    "        targets_batch = [targets_batch[order_[0]] for order_ in order]\n",
    "\n",
    "        inputs_packed = rnn.pack_sequence(inputs_batch)\n",
    "        targets_packed = rnn.pack_sequence(targets_batch)\n",
    "        return inputs_packed, targets_packed\n",
    "    \n",
    "    def test(self, test_data):\n",
    "        with torch.no_grad():\n",
    "            inputs_packed, targets_packed = self._prepare_batch(test_data)\n",
    "            outputs = self.model(inputs_packed, True)\n",
    "        calculate_scores(outputs, targets_packed.data, log=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\t[ 6400/14041]\tTime: 0.06 (0.06)\tLoss: 0.3621 (0.5166)\n",
      "Train Epoch: 1\t[12800/14041]\tTime: 0.08 (0.06)\tLoss: 0.2438 (0.3944)\n",
      "====> Train. 1\tTotal time: 7.09\tAverage loss: 0.3774\n",
      "====> Valid. 1\tTotal time: 0.59\tAverage loss: 0.2344\tF-1: 0.9294\tF-0.5: 0.9294\t\n",
      "Train Epoch: 2\t[ 6400/14041]\tTime: 0.07 (0.07)\tLoss: 0.2409 (0.2012)\n",
      "Train Epoch: 2\t[12800/14041]\tTime: 0.09 (0.07)\tLoss: 0.1753 (0.1864)\n",
      "====> Train. 2\tTotal time: 7.98\tAverage loss: 0.1823\n",
      "====> Valid. 2\tTotal time: 0.61\tAverage loss: 0.2271\tF-1: 0.9344\tF-0.5: 0.9344\t\n",
      "Train Epoch: 3\t[ 6400/14041]\tTime: 0.08 (0.07)\tLoss: 0.1595 (0.1462)\n",
      "Train Epoch: 3\t[12800/14041]\tTime: 0.09 (0.07)\tLoss: 0.1357 (0.1446)\n",
      "====> Train. 3\tTotal time: 8.24\tAverage loss: 0.1434\n",
      "====> Valid. 3\tTotal time: 0.63\tAverage loss: 0.2239\tF-1: 0.9319\tF-0.5: 0.9319\t\n",
      "Train Epoch: 4\t[ 6400/14041]\tTime: 0.07 (0.07)\tLoss: 0.1460 (0.1240)\n",
      "Train Epoch: 4\t[12800/14041]\tTime: 0.08 (0.07)\tLoss: 0.1344 (0.1253)\n",
      "====> Train. 4\tTotal time: 8.01\tAverage loss: 0.1243\n",
      "====> Valid. 4\tTotal time: 0.62\tAverage loss: 0.2218\tF-1: 0.9391\tF-0.5: 0.9391\t\n",
      "Train Epoch: 5\t[ 6400/14041]\tTime: 0.07 (0.07)\tLoss: 0.1681 (0.1129)\n",
      "Train Epoch: 5\t[12800/14041]\tTime: 0.08 (0.07)\tLoss: 0.1137 (0.1145)\n",
      "====> Train. 5\tTotal time: 7.94\tAverage loss: 0.1137\n",
      "====> Valid. 5\tTotal time: 0.61\tAverage loss: 0.2335\tF-1: 0.9336\tF-0.5: 0.9336\t\n",
      "Train Epoch: 6\t[ 6400/14041]\tTime: 0.07 (0.07)\tLoss: 0.1544 (0.1092)\n",
      "Train Epoch: 6\t[12800/14041]\tTime: 0.09 (0.07)\tLoss: 0.1148 (0.1124)\n",
      "====> Train. 6\tTotal time: 7.85\tAverage loss: 0.1113\n",
      "====> Valid. 6\tTotal time: 0.62\tAverage loss: 0.2158\tF-1: 0.9376\tF-0.5: 0.9376\t\n",
      "Train Epoch: 7\t[ 6400/14041]\tTime: 0.07 (0.07)\tLoss: 0.1251 (0.1052)\n",
      "Train Epoch: 7\t[12800/14041]\tTime: 0.08 (0.07)\tLoss: 0.1277 (0.1100)\n",
      "====> Train. 7\tTotal time: 7.63\tAverage loss: 0.1086\n",
      "====> Valid. 7\tTotal time: 0.61\tAverage loss: 0.2179\tF-1: 0.9369\tF-0.5: 0.9369\t\n",
      "Train Epoch: 8\t[ 6400/14041]\tTime: 0.07 (0.07)\tLoss: 0.1161 (0.1097)\n",
      "Train Epoch: 8\t[12800/14041]\tTime: 0.08 (0.07)\tLoss: 0.1376 (0.1138)\n",
      "====> Train. 8\tTotal time: 7.51\tAverage loss: 0.1124\n",
      "====> Valid. 8\tTotal time: 0.61\tAverage loss: 0.2234\tF-1: 0.9377\tF-0.5: 0.9377\t\n",
      "Tag\tSize\tPrecision\tRecall\t\tF1\tF0.5\n",
      "B-ORG\t1661\t0.6609\t\t0.6008\t\t0.6295\t0.6480\n",
      "O\t38323\t0.9733\t\t0.9632\t\t0.9683\t0.9713\n",
      "B-MISC\t702\t0.7509\t\t0.5926\t\t0.6624\t0.7128\n",
      "B-PER\t1617\t0.6675\t\t0.6964\t\t0.6816\t0.6730\n",
      "I-PER\t1156\t0.6859\t\t0.8330\t\t0.7523\t0.7110\n",
      "B-LOC\t1668\t0.7643\t\t0.7602\t\t0.7622\t0.7635\n",
      "I-ORG\t835\t0.3995\t\t0.5928\t\t0.4773\t0.4274\n",
      "I-MISC\t216\t0.5081\t\t0.5833\t\t0.5431\t0.5215\n",
      "I-LOC\t257\t0.5024\t\t0.4086\t\t0.4506\t0.4803\n",
      "MICRO\t46435\t0.9133\t\t0.9133\t\t0.9133\t0.9133\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 64\n",
    "learning_rate = 0.1\n",
    "\n",
    "model = LSTMTagger(embedding_dim=embedding_dim,\n",
    "                   hidden_dim=hidden_dim,\n",
    "                   vocab_size=vocab_size,\n",
    "                   tagset_size=tagset_size,\n",
    "                   pretrained_embeddings=gloves_matrix)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_batch_size = 128\n",
    "val_batch_size = 128\n",
    "\n",
    "trainer = Trainer(model, loss_function, optimizer, word_to_idx, tag_to_idx)\n",
    "trainer.run(8, train_data, train_batch_size, val_data, val_batch_size)\n",
    "trainer.test(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Compare the performances (F1 and F0.5 scores) for each strategy of loading the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy 1\n",
      "Loaded GloVe embeddings as dict vocabulary and embedding matrix of shape (30289, 100)\n",
      "====> Train. 1\tTotal time: 7.07\tAverage loss: 0.4063\n",
      "====> Valid. 1\tTotal time: 0.58\tAverage loss: 0.2833\tF-1: 0.9091\tF-0.5: 0.9091\t\n",
      "====> Train. 2\tTotal time: 7.75\tAverage loss: 0.2401\n",
      "====> Valid. 2\tTotal time: 0.61\tAverage loss: 0.2686\tF-1: 0.9230\tF-0.5: 0.9230\t\n",
      "====> Train. 3\tTotal time: 8.00\tAverage loss: 0.1849\n",
      "====> Valid. 3\tTotal time: 0.61\tAverage loss: 0.2310\tF-1: 0.9277\tF-0.5: 0.9277\t\n",
      "====> Train. 4\tTotal time: 7.80\tAverage loss: 0.1469\n",
      "====> Valid. 4\tTotal time: 0.61\tAverage loss: 0.2344\tF-1: 0.9293\tF-0.5: 0.9293\t\n",
      "====> Train. 5\tTotal time: 7.90\tAverage loss: 0.1323\n",
      "====> Valid. 5\tTotal time: 0.61\tAverage loss: 0.2289\tF-1: 0.9300\tF-0.5: 0.9300\t\n",
      "Tag\tSize\tPrecision\tRecall\t\tF1\tF0.5\n",
      "B-ORG\t1661\t0.5827\t\t0.6466\t\t0.6130\t0.5945\n",
      "O\t38323\t0.9842\t\t0.9548\t\t0.9693\t0.9782\n",
      "B-MISC\t702\t0.7523\t\t0.5755\t\t0.6521\t0.7088\n",
      "B-PER\t1617\t0.7542\t\t0.5881\t\t0.6609\t0.7139\n",
      "I-PER\t1156\t0.5546\t\t0.8304\t\t0.6651\t0.5941\n",
      "B-LOC\t1668\t0.5352\t\t0.8291\t\t0.6505\t0.5761\n",
      "I-ORG\t835\t0.6012\t\t0.5796\t\t0.5902\t0.5968\n",
      "I-MISC\t216\t0.4198\t\t0.4722\t\t0.4444\t0.4293\n",
      "I-LOC\t257\t0.4531\t\t0.4514\t\t0.4522\t0.4528\n",
      "MICRO\t46435\t0.9058\t\t0.9058\t\t0.9058\t0.9058\n",
      "\n",
      "Strategy 2\n",
      "Loaded GloVe embeddings as dict vocabulary and embedding matrix of shape (30289, 100)\n",
      "====> Train. 1\tTotal time: 7.09\tAverage loss: 0.3347\n",
      "====> Valid. 1\tTotal time: 0.58\tAverage loss: 0.1807\tF-1: 0.9515\tF-0.5: 0.9515\t\n",
      "====> Train. 2\tTotal time: 7.88\tAverage loss: 0.1342\n",
      "====> Valid. 2\tTotal time: 0.59\tAverage loss: 0.1704\tF-1: 0.9543\tF-0.5: 0.9543\t\n",
      "====> Train. 3\tTotal time: 8.18\tAverage loss: 0.0942\n",
      "====> Valid. 3\tTotal time: 0.61\tAverage loss: 0.1823\tF-1: 0.9527\tF-0.5: 0.9527\t\n",
      "====> Train. 4\tTotal time: 8.23\tAverage loss: 0.0907\n",
      "====> Valid. 4\tTotal time: 0.62\tAverage loss: 0.1976\tF-1: 0.9484\tF-0.5: 0.9484\t\n",
      "====> Train. 5\tTotal time: 7.99\tAverage loss: 0.0938\n",
      "====> Valid. 5\tTotal time: 0.62\tAverage loss: 0.2015\tF-1: 0.9481\tF-0.5: 0.9481\t\n",
      "Tag\tSize\tPrecision\tRecall\t\tF1\tF0.5\n",
      "B-ORG\t1661\t0.6659\t\t0.7056\t\t0.6852\t0.6735\n",
      "O\t38323\t0.9840\t\t0.9698\t\t0.9768\t0.9811\n",
      "B-MISC\t702\t0.6282\t\t0.6595\t\t0.6435\t0.6342\n",
      "B-PER\t1617\t0.7532\t\t0.7718\t\t0.7624\t0.7568\n",
      "I-PER\t1156\t0.7624\t\t0.9161\t\t0.8322\t0.7889\n",
      "B-LOC\t1668\t0.7288\t\t0.7944\t\t0.7602\t0.7411\n",
      "I-ORG\t835\t0.5761\t\t0.5940\t\t0.5849\t0.5796\n",
      "I-MISC\t216\t0.5275\t\t0.5324\t\t0.5300\t0.5285\n",
      "I-LOC\t257\t0.6062\t\t0.5331\t\t0.5673\t0.5900\n",
      "MICRO\t46435\t0.9299\t\t0.9299\t\t0.9299\t0.9299\n",
      "\n",
      "Strategy 3\n",
      "Loaded GloVe embeddings as dict vocabulary and embedding matrix of shape (30289, 100)\n",
      "====> Train. 1\tTotal time: 7.07\tAverage loss: 0.3214\n",
      "====> Valid. 1\tTotal time: 0.57\tAverage loss: 0.1608\tF-1: 0.9571\tF-0.5: 0.9571\t\n",
      "====> Train. 2\tTotal time: 7.69\tAverage loss: 0.1168\n",
      "====> Valid. 2\tTotal time: 0.59\tAverage loss: 0.1558\tF-1: 0.9596\tF-0.5: 0.9596\t\n",
      "====> Train. 3\tTotal time: 8.16\tAverage loss: 0.0853\n",
      "====> Valid. 3\tTotal time: 0.61\tAverage loss: 0.1743\tF-1: 0.9545\tF-0.5: 0.9545\t\n",
      "====> Train. 4\tTotal time: 8.23\tAverage loss: 0.0814\n",
      "====> Valid. 4\tTotal time: 0.61\tAverage loss: 0.1961\tF-1: 0.9507\tF-0.5: 0.9507\t\n",
      "====> Train. 5\tTotal time: 7.93\tAverage loss: 0.0835\n",
      "====> Valid. 5\tTotal time: 0.61\tAverage loss: 0.2044\tF-1: 0.9474\tF-0.5: 0.9474\t\n",
      "Tag\tSize\tPrecision\tRecall\t\tF1\tF0.5\n",
      "B-ORG\t1661\t0.6184\t\t0.6869\t\t0.6509\t0.6310\n",
      "O\t38323\t0.9858\t\t0.9638\t\t0.9747\t0.9813\n",
      "B-MISC\t702\t0.7090\t\t0.6524\t\t0.6795\t0.6969\n",
      "B-PER\t1617\t0.7823\t\t0.8312\t\t0.8060\t0.7916\n",
      "I-PER\t1156\t0.8585\t\t0.8555\t\t0.8570\t0.8579\n",
      "B-LOC\t1668\t0.6920\t\t0.8633\t\t0.7682\t0.7206\n",
      "I-ORG\t835\t0.5319\t\t0.6898\t\t0.6006\t0.5574\n",
      "I-MISC\t216\t0.5392\t\t0.5417\t\t0.5404\t0.5397\n",
      "I-LOC\t257\t0.6502\t\t0.5642\t\t0.6042\t0.6310\n",
      "MICRO\t46435\t0.9292\t\t0.9292\t\t0.9292\t0.9292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for strategy in [1, 2, 3]:\n",
    "    print('Strategy', strategy)\n",
    "    \n",
    "    embedding_dim = 100\n",
    "\n",
    "    gloves, gloves_matrix = get_gloves(word_to_idx.keys(), glove, strategy)\n",
    "    word_to_glove = dict(zip(word_to_idx.keys(), gloves))\n",
    "    print('Loaded GloVe embeddings as dict vocabulary and embedding matrix of shape',\n",
    "              tuple(gloves_matrix.shape))\n",
    "    \n",
    "    hidden_dim = 64\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    model = LSTMTagger(embedding_dim=embedding_dim,\n",
    "                       hidden_dim=hidden_dim,\n",
    "                       vocab_size=vocab_size,\n",
    "                       tagset_size=tagset_size,\n",
    "                       pretrained_embeddings=gloves_matrix)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_batch_size = 128\n",
    "    val_batch_size = 128\n",
    "\n",
    "    trainer = Trainer(model, loss_function, optimizer, word_to_idx, tag_to_idx)\n",
    "    trainer.run(5, train_data, train_batch_size, val_data, val_batch_size, log_interval=200)\n",
    "    trainer.test(test_data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The performance quite good for all strategies, however the last two perfom better straightway."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
