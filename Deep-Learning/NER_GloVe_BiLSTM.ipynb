{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition with GloVe-BiLSTM-Softmax model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as progress\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import rnn\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Read and process NER 2003 English Shared Task data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fpath):\n",
    "    \"\"\" Read NER 2003 English Shared Task data (CoNNL file format)\n",
    "    Data format:\n",
    "        All data files contain one word per line with empty lines\n",
    "        representing sentence boundaries. At the end of each line there is \n",
    "        tag which states whether the current word is inside a named entity or not.\n",
    "        The tag also encodes the type of named entity. Example\n",
    "        Example:\n",
    "            U.N. NNP I-NP I-ORG\n",
    "        Each line contains four fields:\n",
    "            [word] [POS tag] [chunk tag] [NE tag]\n",
    "            \n",
    "        Four different types of named entities: PERSON, LOCATION, ORGANIZATION, MISC.\n",
    "        \n",
    "    Args:\n",
    "        fpath: path to data file\n",
    "    Returns:\n",
    "        sentences_words: list of sentences' words (one sentence is a list of words)\n",
    "        sentences_tags: list of sentences' tags (one sentence is a list of tags corresponding to words)\n",
    "    \"\"\"\n",
    "    with open(fpath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    raw_sentences = []\n",
    "    i_prev, i_next = 0, 0\n",
    "    while i_next < len(lines):\n",
    "        if lines[i_next] == '\\n':\n",
    "            raw_sentences.append(lines[i_prev:i_next])\n",
    "            i_prev = i_next + 1\n",
    "        i_next += 1\n",
    "\n",
    "    sentences_words = []\n",
    "    sentences_tags = []\n",
    "    for sentence in raw_sentences:\n",
    "        words = [string.split()[0] for string in sentence]\n",
    "        tags = [string.split()[3] for string in sentence]\n",
    "        if words != ['-DOCSTART-']:\n",
    "            sentences_words.append(words)\n",
    "            sentences_tags.append(tags)\n",
    "    return list(zip(sentences_words, sentences_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train (14041), dev (3250), test (3453) data\n"
     ]
    }
   ],
   "source": [
    "root = './data'\n",
    "\n",
    "train_data = read_data('./data/train.txt')\n",
    "val_data = read_data('./data/dev.txt')\n",
    "test_data = read_data('./data/test.txt')\n",
    "\n",
    "print('Loaded train ({}), dev ({}), test ({}) data'.format(len(train_data),\n",
    "                                                          len(val_data),\n",
    "                                                          len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30289 words in vocabulary\n",
      "9 tags (IOB2 tagging scheme): dict_keys(['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC'])\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {}  # vocabulary\n",
    "tag_to_idx = {}  # tagset\n",
    "\n",
    "for i, (sentence, tags) in enumerate(train_data + val_data + test_data):\n",
    "    for word in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_idx:\n",
    "            tag_to_idx[tag] = len(tag_to_idx)\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "print(vocab_size, 'words in vocabulary')\n",
    "\n",
    "tagset_size = len(tag_to_idx)\n",
    "print(tagset_size, 'tags (IOB2 tagging scheme):', tag_to_idx.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccda1e6e5bde4c329e1137cc3d714924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GloVe for 400000 words loaded\n"
     ]
    }
   ],
   "source": [
    "with open('./glove/glove.6B.100d.txt', 'r') as f:\n",
    "    glove_raw = f.readlines()\n",
    "    \n",
    "glove = {line.split()[0]: torch.tensor([float(val) for val in line.split()[1:]])\n",
    "         for line in progress(glove_raw)}\n",
    "\n",
    "print('GloVe for {} words loaded'.format(len(glove)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe embeddings as dict vocabulary and embedding matrix of shape (30289, 100)\n",
      "Example embedding for `the`:\n",
      " tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n",
      "        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n",
      "         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n",
      "         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n",
      "         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n",
      "        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n",
      "        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n",
      "         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n",
      "         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n",
      "        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n",
      "         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n",
      "         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n",
      "        -0.5203, -0.1459,  0.8278,  0.2706])\n"
     ]
    }
   ],
   "source": [
    "def get_gloves(seq, glove, strategy=3):\n",
    "    \"\"\"Get Global Vectors for Word Representation in a list of words\n",
    "    Args:\n",
    "        seq: list of words of size N\n",
    "        glove: GloVe dictionary of 400000 words, the format\n",
    "               {word: 100D vector representation}\n",
    "        strategy: 1, 2 or 3\n",
    "            1 -- load the embeddings for original capitalization of words.\n",
    "                 If embedding for this word doesn’t exists, associate\n",
    "                 it with <UNK> embedding.\n",
    "            2 -- load the embeddings for lowercased capitalization of words.\n",
    "                 If embedding for this lowercased word doesn’t exists,\n",
    "                 associate it with <UNK> embedding.\n",
    "            3 -- Для різної капіталізації слів завантажуємо одні і ті самі (lowercased)\n",
    "                 вектори ембедінгів, але в нашому словнику це різні слова.\n",
    "                 Тобто, “Hello” та “hello” відповідають фізично два вектори,\n",
    "                 які ідентичні за своїми значеннями перед тренуванням\n",
    "                 (якщо freeze_embeddings стоїть  False, то в процесі\n",
    "                 тренування вони будуть змінюватись).\n",
    "    Returns:\n",
    "        out_seq: a list of [100] tensors (GloVes) of size N\n",
    "    \"\"\"\n",
    "    assert strategy in [1,2,3]\n",
    "    \n",
    "    def associate_embedding(word):\n",
    "        if strategy == 1:\n",
    "            return glove['unk'] if word not in glove else glove[word]\n",
    "        if strategy == 2:\n",
    "            return glove['unk'] if word.lower() not in glove else glove[word.lower()]\n",
    "        return glove['unk'] if word.lower() not in glove else \\\n",
    "                glove[word.lower()] if word not in glove else glove[word]\n",
    "    \n",
    "    out_seq = [associate_embedding(w) for w in seq]\n",
    "    return out_seq, torch.stack(out_seq, 0)\n",
    "\n",
    "embedding_dim = 100\n",
    "strategy = 3\n",
    "\n",
    "gloves, gloves_matrix = get_gloves(word_to_idx.keys(), glove, strategy)\n",
    "word_to_glove = dict(zip(word_to_idx.keys(), gloves))\n",
    "print('Loaded GloVe embeddings as dict vocabulary and embedding matrix of shape',\n",
    "          tuple(gloves_matrix.shape))\n",
    "\n",
    "print('Example embedding for `the`:\\n', word_to_glove['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-4. Train BiLSTM model on batches & test with micro-average Precision/Recall/F1/F0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        embedding_dim -- 100\n",
    "        hidden_dim -- hidden state dimensionality\n",
    "        vocab_size -- vocabulary size\n",
    "        tagset_size -- tag set size\n",
    "        pretrained_embeddings -- None or [vocab_size, embedding_dim] tensor\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size,\n",
    "                 pretrained_embeddings=None, strategy=1):\n",
    "\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.word_embeddings = nn.Embedding.from_pretrained(pretrained_embeddings,\n",
    "                                                                freeze=strategy!=3)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2,\n",
    "                            bidirectional=True, batch_first=True)\n",
    "        self.tagger = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs, packed_inputs=False):\n",
    "        if packed_inputs:\n",
    "            embeds = rnn.PackedSequence(self.word_embeddings(inputs.data), inputs.batch_sizes)\n",
    "        else:\n",
    "            embeds = self.word_embeddings(inputs).view(len(inputs), 1, -1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "\n",
    "        if packed_inputs:\n",
    "            tag_space = self.tagger(lstm_out.data)\n",
    "        else:\n",
    "            tag_space = self.tagger(lstm_out.view(len(inputs), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def seq_to_idxs(seq, mapping):\n",
    "    \"\"\"Converts a sequence of elements to sequence of indices\n",
    "       using given mapping\n",
    "\n",
    "    Args:\n",
    "        seq -- list of elements\n",
    "        mapping -- {element: idx} dict\n",
    "    \"\"\"\n",
    "    outs_seq = [torch.tensor(mapping[el], dtype=torch.long) for el in seq]\n",
    "    outs_seq = torch.stack(outs_seq, 0)\n",
    "    return outs_seq\n",
    "\n",
    "\n",
    "def calculate_scores(outputs, targets, log=False):\n",
    "    \"\"\"Calculate per-class and micro-average precision, recall, F-1 score and F-0.5 score\n",
    "    \n",
    "        Args:\n",
    "            outputs: tensor of log-softmax model outputs for the dataset\n",
    "            targets: tensor of true tag indices for the dataset\n",
    "            log: whether to print or not\n",
    "        Returns:\n",
    "            (micro-average) precision, recall, F-1 score, F-0.5 score\n",
    "    \"\"\"    \n",
    "    def F_score(precision, recall, beta=1):\n",
    "        return (1 + beta**2) * precision * recall / ((beta**2) * precision + recall + 1e-15)\n",
    "\n",
    "    pred = outputs.max(dim=1)[1]\n",
    "    TP, TN = torch.zeros(9), torch.zeros(9)\n",
    "    FP, FN = torch.zeros(9), torch.zeros(9)\n",
    "    if log:\n",
    "        print('Tag\\tSize\\tPrecision\\tRecall\\t\\tF1\\tF0.5')\n",
    "    for tag in tag_to_idx.keys():\n",
    "        i = tag_to_idx[tag]\n",
    "        TP[i] = (pred[targets==i]==i).sum()\n",
    "        TN[i] = (pred[targets!=i]!=i).sum()\n",
    "        FP[i] = (pred[targets!=i]==i).sum()\n",
    "        FN[i] = (pred[targets==i]!=i).sum()\n",
    "\n",
    "        precision = float(TP[i] / (TP[i] + FP[i] + 1e-15))\n",
    "        recall = float(TP[i] / (TP[i] + FN[i] + 1e-15))\n",
    "        if log:\n",
    "            print('{}\\t{}\\t{:.4f}\\t\\t{:.4f}\\t\\t{:.4f}\\t{:.4f}'.format(tag, len(targets[targets==i]),\n",
    "                                                                    precision, recall,\n",
    "                                                                    F_score(precision, recall, beta=1),\n",
    "                                                                    F_score(precision, recall, beta=0.5)))\n",
    "\n",
    "    MicroAvePrecision = float(TP.sum() / (TP.sum() + FP.sum()))\n",
    "    MicroAveRecall = float(TP.sum() / (TP.sum() + FN.sum()))\n",
    "    F1 = F_score(MicroAvePrecision, MicroAveRecall, beta=1)\n",
    "    F05 = F_score(MicroAvePrecision, MicroAveRecall, beta=0.5)\n",
    "    if log:\n",
    "        print('{}\\t{}\\t{:.4f}\\t\\t{:.4f}\\t\\t{:.4f}\\t{:.4f}'.format('MICRO', len(targets),\n",
    "                                                                  MicroAvePrecision,\n",
    "                                                                  MicroAveRecall,\n",
    "                                                                  F1, F05))\n",
    "    return MicroAvePrecision, MicroAveRecall, F1, F05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, loss_fn, optimizer, word_to_idx, tag_to_idx):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "    \n",
    "    def run(self, num_epochs,\n",
    "            train_data, train_batch_size, val_data, val_batch_size,\n",
    "            log_interval=50):\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            self.__train(epoch, train_data, train_batch_size, log_interval)\n",
    "            self.__validate(epoch, val_data, val_batch_size)\n",
    "        \n",
    "    def __train(self, epoch, train_data, batch_size, log_interval=50):\n",
    "        losses = AverageMeter()\n",
    "        batch_time = AverageMeter()\n",
    "        batch_start = 0\n",
    "        batch_idx = 0\n",
    "        while batch_start < len(train_data):\n",
    "            end = time.time()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            batch = train_data[batch_start:batch_start + batch_size]\n",
    "            inputs_packed, targets_packed = self._prepare_batch(batch)\n",
    "            outputs = self.model(inputs_packed, True)\n",
    "\n",
    "            loss = self.loss_fn(outputs, targets_packed.data)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            losses.update(loss.item())\n",
    "            batch_time.update(time.time() - end)\n",
    "\n",
    "            batch_start += batch_size\n",
    "            batch_idx += 1\n",
    "            \n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {}\\t[{:>5}/{:<5}]\\tTime: {:.2f} ({:.2f})\\tLoss: {:.4f} ({:.4f})'.format(epoch,\n",
    "                    min(batch_start, len(train_data)), len(train_data),\n",
    "                    batch_time.val, batch_time.avg,\n",
    "                    losses.val, losses.avg))\n",
    "        print('====> Train. {}\\tTotal time: {:.2f}\\tAverage loss: {:.4f}'.format(\n",
    "              epoch, batch_time.sum, losses.avg))\n",
    "            \n",
    "            \n",
    "    def __validate(self, epoch, val_data, batch_size):\n",
    "        losses = AverageMeter()\n",
    "        batch_time = AverageMeter()\n",
    "        F1scores = AverageMeter()\n",
    "        F05scores = AverageMeter()\n",
    "        with torch.no_grad():\n",
    "            batch_start = 0\n",
    "            batch_idx = 0\n",
    "            while batch_start < len(val_data):\n",
    "                end = time.time()\n",
    "\n",
    "                batch = val_data[batch_start:batch_start + batch_size]\n",
    "                inputs_packed, targets_packed = self._prepare_batch(batch)\n",
    "                outputs = self.model(inputs_packed, True)\n",
    "\n",
    "                loss = self.loss_fn(outputs, targets_packed.data)\n",
    "                _, _, F1, F05 = calculate_scores(outputs, targets_packed.data, log=False)\n",
    "                F1scores.update(F1)\n",
    "                F05scores.update(F05)\n",
    "\n",
    "                losses.update(loss.item())\n",
    "                batch_time.update(time.time() - end)\n",
    "\n",
    "                batch_start += batch_size\n",
    "                batch_idx += 1\n",
    "        print('====> Valid. {}\\tTotal time: {:.2f}\\tAverage loss: {:.4f}\\tF-1: {:.4f}\\tF-0.5: {:.4f}\\t'.format(\n",
    "              epoch, batch_time.sum, losses.avg, F1scores.avg, F05scores.avg))\n",
    "            \n",
    "    def _prepare_batch(self, batch):\n",
    "        inputs_batch = [seq_to_idxs(seq[0], self.word_to_idx) for seq in batch]\n",
    "        targets_batch = [seq_to_idxs(seq[1], self.tag_to_idx) for seq in batch]\n",
    "\n",
    "        order = sorted(enumerate(inputs_batch), key=lambda x: len(x[1]), reverse=True)\n",
    "        inputs_batch = [inputs_batch[order_[0]] for order_ in order]\n",
    "        targets_batch = [targets_batch[order_[0]] for order_ in order]\n",
    "\n",
    "        inputs_packed = rnn.pack_sequence(inputs_batch)\n",
    "        targets_packed = rnn.pack_sequence(targets_batch)\n",
    "        return inputs_packed, targets_packed\n",
    "    \n",
    "    def test(self, test_data):\n",
    "        with torch.no_grad():\n",
    "            inputs_packed, targets_packed = self._prepare_batch(test_data)\n",
    "            outputs = self.model(inputs_packed, True)\n",
    "        calculate_scores(outputs, targets_packed.data, log=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\t[ 6400/14041]\tTime: 0.07 (0.07)\tLoss: 0.1615 (0.2438)\n",
      "Train Epoch: 1\t[12800/14041]\tTime: 0.09 (0.07)\tLoss: 0.1274 (0.1818)\n",
      "====> Train. 1\tTotal time: 8.05\tAverage loss: 0.1747\n",
      "====> Valid. 1\tTotal time: 0.62\tAverage loss: 0.2585\tF-1: 0.9333\tF-0.5: 0.9333\t\n",
      "Train Epoch: 2\t[ 6400/14041]\tTime: 0.07 (0.07)\tLoss: 0.1114 (0.0981)\n",
      "Train Epoch: 2\t[12800/14041]\tTime: 0.09 (0.07)\tLoss: 0.1156 (0.1006)\n",
      "====> Train. 2\tTotal time: 8.13\tAverage loss: 0.0997\n",
      "====> Valid. 2\tTotal time: 0.61\tAverage loss: 0.2370\tF-1: 0.9401\tF-0.5: 0.9401\t\n",
      "Train Epoch: 3\t[ 6400/14041]\tTime: 0.07 (0.07)\tLoss: 0.1196 (0.0871)\n",
      "Train Epoch: 3\t[12800/14041]\tTime: 0.09 (0.07)\tLoss: 0.1169 (0.0910)\n",
      "====> Train. 3\tTotal time: 8.07\tAverage loss: 0.0904\n",
      "====> Valid. 3\tTotal time: 0.62\tAverage loss: 0.2625\tF-1: 0.9344\tF-0.5: 0.9344\t\n",
      "Train Epoch: 4\t[ 6400/14041]\tTime: 0.07 (0.07)\tLoss: 0.0853 (0.0831)\n",
      "Train Epoch: 4\t[12800/14041]\tTime: 0.08 (0.07)\tLoss: 0.0968 (0.0856)\n",
      "====> Train. 4\tTotal time: 7.86\tAverage loss: 0.0853\n",
      "====> Valid. 4\tTotal time: 0.62\tAverage loss: 0.2278\tF-1: 0.9451\tF-0.5: 0.9451\t\n",
      "Train Epoch: 5\t[ 6400/14041]\tTime: 0.07 (0.07)\tLoss: 0.1128 (0.0830)\n",
      "Train Epoch: 5\t[12800/14041]\tTime: 0.08 (0.07)\tLoss: 0.1257 (0.0865)\n",
      "====> Train. 5\tTotal time: 7.66\tAverage loss: 0.0857\n",
      "====> Valid. 5\tTotal time: 0.60\tAverage loss: 0.2330\tF-1: 0.9416\tF-0.5: 0.9416\t\n",
      "Train Epoch: 6\t[ 6400/14041]\tTime: 0.06 (0.07)\tLoss: 0.0882 (0.0845)\n",
      "Train Epoch: 6\t[12800/14041]\tTime: 0.09 (0.07)\tLoss: 0.1168 (0.0855)\n",
      "====> Train. 6\tTotal time: 7.65\tAverage loss: 0.0845\n",
      "====> Valid. 6\tTotal time: 0.61\tAverage loss: 0.2325\tF-1: 0.9428\tF-0.5: 0.9428\t\n",
      "Train Epoch: 7\t[ 6400/14041]\tTime: 0.06 (0.06)\tLoss: 0.0815 (0.0869)\n",
      "Train Epoch: 7\t[12800/14041]\tTime: 0.08 (0.07)\tLoss: 0.1057 (0.0894)\n",
      "====> Train. 7\tTotal time: 7.40\tAverage loss: 0.0887\n",
      "====> Valid. 7\tTotal time: 0.61\tAverage loss: 0.2409\tF-1: 0.9425\tF-0.5: 0.9425\t\n",
      "Train Epoch: 8\t[ 6400/14041]\tTime: 0.06 (0.07)\tLoss: 0.0993 (0.0943)\n",
      "Train Epoch: 8\t[12800/14041]\tTime: 0.08 (0.07)\tLoss: 0.1109 (0.0954)\n",
      "====> Train. 8\tTotal time: 7.45\tAverage loss: 0.0942\n",
      "====> Valid. 8\tTotal time: 0.60\tAverage loss: 0.2366\tF-1: 0.9431\tF-0.5: 0.9431\t\n",
      "Tag\tSize\tPrecision\tRecall\t\tF1\tF0.5\n",
      "B-ORG\t1661\t0.6184\t\t0.6761\t\t0.6460\t0.6291\n",
      "O\t38323\t0.9804\t\t0.9622\t\t0.9713\t0.9767\n",
      "B-MISC\t702\t0.6456\t\t0.5684\t\t0.6045\t0.6285\n",
      "B-PER\t1617\t0.8468\t\t0.7044\t\t0.7691\t0.8139\n",
      "I-PER\t1156\t0.7843\t\t0.8806\t\t0.8297\t0.8018\n",
      "B-LOC\t1668\t0.6564\t\t0.8393\t\t0.7366\t0.6863\n",
      "I-ORG\t835\t0.5305\t\t0.6455\t\t0.5824\t0.5501\n",
      "I-MISC\t216\t0.4124\t\t0.5556\t\t0.4734\t0.4348\n",
      "I-LOC\t257\t0.5621\t\t0.6693\t\t0.6110\t0.5807\n",
      "MICRO\t46435\t0.9214\t\t0.9214\t\t0.9214\t0.9214\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 64\n",
    "learning_rate = 0.1\n",
    "\n",
    "model = LSTMTagger(embedding_dim=embedding_dim,\n",
    "                   hidden_dim=hidden_dim,\n",
    "                   vocab_size=vocab_size,\n",
    "                   tagset_size=tagset_size,\n",
    "                   pretrained_embeddings=gloves_matrix,\n",
    "                   strategy=3)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_batch_size = 128\n",
    "val_batch_size = 128\n",
    "\n",
    "trainer = Trainer(model, loss_function, optimizer, word_to_idx, tag_to_idx)\n",
    "trainer.run(8, train_data, train_batch_size, val_data, val_batch_size)\n",
    "trainer.test(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Compare the performances (F1 and F0.5 scores) for each strategy of loading the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy 1\n",
      "Loaded GloVe embeddings as dict vocabulary and embedding matrix of shape (30289, 100)\n",
      "====> Train. 1\tTotal time: 4.68\tAverage loss: 0.5169\n",
      "====> Valid. 1\tTotal time: 0.58\tAverage loss: 0.4581\tF-1: 0.8290\tF-0.5: 0.8290\t\n",
      "====> Train. 2\tTotal time: 4.83\tAverage loss: 0.4432\n",
      "====> Valid. 2\tTotal time: 0.57\tAverage loss: 0.4529\tF-1: 0.8452\tF-0.5: 0.8452\t\n",
      "====> Train. 3\tTotal time: 5.19\tAverage loss: 0.4540\n",
      "====> Valid. 3\tTotal time: 0.59\tAverage loss: 0.4684\tF-1: 0.8477\tF-0.5: 0.8477\t\n",
      "====> Train. 4\tTotal time: 5.07\tAverage loss: 0.4350\n",
      "====> Valid. 4\tTotal time: 0.57\tAverage loss: 0.4518\tF-1: 0.8560\tF-0.5: 0.8560\t\n",
      "====> Train. 5\tTotal time: 5.23\tAverage loss: 0.4456\n",
      "====> Valid. 5\tTotal time: 0.57\tAverage loss: 0.4452\tF-1: 0.8569\tF-0.5: 0.8569\t\n",
      "Tag\tSize\tPrecision\tRecall\t\tF1\tF0.5\n",
      "B-ORG\t1661\t0.2868\t\t0.0704\t\t0.1131\t0.1776\n",
      "O\t38323\t0.9124\t\t0.9811\t\t0.9455\t0.9254\n",
      "B-MISC\t702\t0.5000\t\t0.0028\t\t0.0057\t0.0139\n",
      "B-PER\t1617\t0.8904\t\t0.0402\t\t0.0769\t0.1702\n",
      "I-PER\t1156\t0.7136\t\t0.6704\t\t0.6913\t0.7045\n",
      "B-LOC\t1668\t0.3282\t\t0.5695\t\t0.4164\t0.3585\n",
      "I-ORG\t835\t0.2622\t\t0.2251\t\t0.2423\t0.2538\n",
      "I-MISC\t216\t0.0000\t\t0.0000\t\t0.0000\t0.0000\n",
      "I-LOC\t257\t0.2000\t\t0.0350\t\t0.0596\t0.1030\n",
      "MICRO\t46435\t0.8550\t\t0.8550\t\t0.8550\t0.8550\n",
      "\n",
      "Strategy 2\n",
      "Loaded GloVe embeddings as dict vocabulary and embedding matrix of shape (30289, 100)\n",
      "====> Train. 1\tTotal time: 4.59\tAverage loss: 0.3897\n",
      "====> Valid. 1\tTotal time: 0.57\tAverage loss: 0.2256\tF-1: 0.9407\tF-0.5: 0.9407\t\n",
      "====> Train. 2\tTotal time: 4.66\tAverage loss: 0.2035\n",
      "====> Valid. 2\tTotal time: 0.57\tAverage loss: 0.1844\tF-1: 0.9522\tF-0.5: 0.9522\t\n",
      "====> Train. 3\tTotal time: 4.77\tAverage loss: 0.1679\n",
      "====> Valid. 3\tTotal time: 0.57\tAverage loss: 0.1742\tF-1: 0.9550\tF-0.5: 0.9550\t\n",
      "====> Train. 4\tTotal time: 4.85\tAverage loss: 0.1502\n",
      "====> Valid. 4\tTotal time: 0.57\tAverage loss: 0.1645\tF-1: 0.9565\tF-0.5: 0.9565\t\n",
      "====> Train. 5\tTotal time: 5.01\tAverage loss: 0.1356\n",
      "====> Valid. 5\tTotal time: 0.57\tAverage loss: 0.1619\tF-1: 0.9582\tF-0.5: 0.9582\t\n",
      "Tag\tSize\tPrecision\tRecall\t\tF1\tF0.5\n",
      "B-ORG\t1661\t0.7813\t\t0.7140\t\t0.7461\t0.7668\n",
      "O\t38323\t0.9713\t\t0.9895\t\t0.9804\t0.9749\n",
      "B-MISC\t702\t0.7299\t\t0.6581\t\t0.6921\t0.7143\n",
      "B-PER\t1617\t0.9311\t\t0.8435\t\t0.8851\t0.9121\n",
      "I-PER\t1156\t0.9339\t\t0.8927\t\t0.9129\t0.9254\n",
      "B-LOC\t1668\t0.8538\t\t0.8543\t\t0.8541\t0.8539\n",
      "I-ORG\t835\t0.6758\t\t0.5018\t\t0.5759\t0.6320\n",
      "I-MISC\t216\t0.6692\t\t0.4028\t\t0.5029\t0.5910\n",
      "I-LOC\t257\t0.6693\t\t0.6615\t\t0.6654\t0.6677\n",
      "MICRO\t46435\t0.9490\t\t0.9490\t\t0.9490\t0.9490\n",
      "\n",
      "Strategy 3\n",
      "Loaded GloVe embeddings as dict vocabulary and embedding matrix of shape (30289, 100)\n",
      "====> Train. 1\tTotal time: 7.14\tAverage loss: 0.3216\n",
      "====> Valid. 1\tTotal time: 0.57\tAverage loss: 0.1689\tF-1: 0.9558\tF-0.5: 0.9558\t\n",
      "====> Train. 2\tTotal time: 7.92\tAverage loss: 0.1215\n",
      "====> Valid. 2\tTotal time: 0.60\tAverage loss: 0.1693\tF-1: 0.9564\tF-0.5: 0.9564\t\n",
      "====> Train. 3\tTotal time: 8.26\tAverage loss: 0.0865\n",
      "====> Valid. 3\tTotal time: 0.63\tAverage loss: 0.1727\tF-1: 0.9548\tF-0.5: 0.9548\t\n",
      "====> Train. 4\tTotal time: 8.31\tAverage loss: 0.0839\n",
      "====> Valid. 4\tTotal time: 0.61\tAverage loss: 0.1832\tF-1: 0.9552\tF-0.5: 0.9552\t\n",
      "====> Train. 5\tTotal time: 8.16\tAverage loss: 0.0903\n",
      "====> Valid. 5\tTotal time: 0.61\tAverage loss: 0.1882\tF-1: 0.9550\tF-0.5: 0.9550\t\n",
      "Tag\tSize\tPrecision\tRecall\t\tF1\tF0.5\n",
      "B-ORG\t1661\t0.6565\t\t0.7158\t\t0.6849\t0.6676\n",
      "O\t38323\t0.9802\t\t0.9804\t\t0.9803\t0.9803\n",
      "B-MISC\t702\t0.6137\t\t0.7037\t\t0.6556\t0.6298\n",
      "B-PER\t1617\t0.8605\t\t0.7477\t\t0.8001\t0.8353\n",
      "I-PER\t1156\t0.8600\t\t0.8824\t\t0.8711\t0.8644\n",
      "B-LOC\t1668\t0.8272\t\t0.8124\t\t0.8197\t0.8242\n",
      "I-ORG\t835\t0.6258\t\t0.6048\t\t0.6151\t0.6215\n",
      "I-MISC\t216\t0.6117\t\t0.5833\t\t0.5972\t0.6058\n",
      "I-LOC\t257\t0.6210\t\t0.5992\t\t0.6099\t0.6165\n",
      "MICRO\t46435\t0.9394\t\t0.9394\t\t0.9394\t0.9394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for strategy in [1, 2, 3]:\n",
    "    print('Strategy', strategy)\n",
    "    \n",
    "    embedding_dim = 100\n",
    "\n",
    "    gloves, gloves_matrix = get_gloves(word_to_idx.keys(), glove, strategy)\n",
    "    word_to_glove = dict(zip(word_to_idx.keys(), gloves))\n",
    "    print('Loaded GloVe embeddings as dict vocabulary and embedding matrix of shape',\n",
    "              tuple(gloves_matrix.shape))\n",
    "    \n",
    "    hidden_dim = 64\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    model = LSTMTagger(embedding_dim=embedding_dim,\n",
    "                       hidden_dim=hidden_dim,\n",
    "                       vocab_size=vocab_size,\n",
    "                       tagset_size=tagset_size,\n",
    "                       pretrained_embeddings=gloves_matrix,\n",
    "                       strategy=strategy)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_batch_size = 128\n",
    "    val_batch_size = 128\n",
    "\n",
    "    trainer = Trainer(model, loss_function, optimizer, word_to_idx, tag_to_idx)\n",
    "    trainer.run(5, train_data, train_batch_size, val_data, val_batch_size, log_interval=200)\n",
    "    trainer.test(test_data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The performance is quite good for all strategies, however the last two perfom better straightway."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
